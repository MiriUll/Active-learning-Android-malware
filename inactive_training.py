import os, glob, sys
from random import shuffle
import numpy as np
from config import *
sys.path.insert(0, aion_path)
import Aion
from Aion.conf.config import *
Aion.conf.config.LOG_FILE = aion_log_file
from Aion.data_inference.learning import ScikitLearners
from Aion.utils.db import *

#source_code_dir = 'C:/Users/manschuetz/PycharmProjects/malewaredetection/utils/source_codes'
source_code_dir = '/home/anschutzm/PycharmProjects/malewaredetection/utils/source_codes/'


def main():
    training_data, test_data = [], []
    # training_data += generate_labeled_list('/home/miriam/malewaredetection/utils/source_codes/s.home.aesalem.Research.Android.Datasets.Goodware.google_play.', 0)
    training_data += generate_labeled_list(source_code_dir, 0)
    training_data += generate_labeled_list(source_code_dir, 1)
    shuffle(training_data)

    test_data += generate_labeled_list(source_code_dir, 0)

    train(training_data, test_data)


def train(training_data, test_data):
    prettyPrint("Welcome to the \"Aion\"'s standard experiment I")
    Xtr, ytr = [], []
    for t, y in training_data:
        Xtr.append(t)
        ytr.append(y)
    Xte, yte = [], []
    for t, y in test_data:
        Xte.append(t)
        yte.append(y)

    metricsDict, metricsDict_test = {}, {}
    ####################################
    # Ensemble of learning algorithms #
    ###################################
    prettyPrint("Ensemble mode classification: K-NN, SVM, and Random Forests")
    predicted_trees100 = []
    # Classifying using K-nearest neighbors
    K = [10]
    #K = [10, 25, 50, 100, 250, 500]
    for k in K:
        prettyPrint("Classifying using K-nearest neighbors with K=%s" % k)
        clf, predicted, predicted_test = ScikitLearners.predictAndTestKNN(Xtr, ytr, Xte, yte, K=k)
        metrics = ScikitLearners.calculateMetrics(ytr, predicted)
        metrics_test = ScikitLearners.calculateMetrics(yte, predicted_test)
        metricsDict["KNN%s" % k] = metrics
        metricsDict_test["KNN%s" % k] = metrics_test

    # Classifying using Random Forests
    E = [10, 25, 50, 75, 100]
    for e in E:
        prettyPrint("Classifying using Random Forests with %s estimators" % e)
        clf, predicted, predicted_test = ScikitLearners.predictAndTestRandomForest(Xtr, ytr, Xte, yte,estimators=e)
        if e == 100:
            predicted_trees100 = predicted_test
        metrics = ScikitLearners.calculateMetrics(ytr, predicted)
        metrics_test = ScikitLearners.calculateMetrics(yte, predicted_test)
        metricsDict["Trees%s" % e] = metrics
        metricsDict_test["Trees%s" % e] = metrics_test

    # Classifying using SVM
    prettyPrint("Classifying using Support vector machines")
    clf, predicted, predicted_test = ScikitLearners.predictAndTestSVM(Xtr, ytr, Xte, yte)
    metrics = ScikitLearners.calculateMetrics(ytr, predicted)
    metrics_test = ScikitLearners.calculateMetrics(yte, predicted_test)
    metricsDict["SVM"] = metrics
    metricsDict_test["SVM"] = metrics_test

    # Now do the majority voting ensemble
    allCs = ["KNN-%s" % x for x in K] + ["FOREST-%s" % e for e in E] + ["SVM"]
    clf, predicted, predicted_test = ScikitLearners.predictAndTestEnsemble(Xtr, ytr, Xte, yte,  classifiers=allCs)
    metrics = ScikitLearners.calculateMetrics(predicted, ytr)  # Used to decide upon whether to iterate more
    metrics_test = ScikitLearners.calculateMetrics(predicted_test, yte)
    metricsDict["Ensemble"] = metrics
    metricsDict_test["Ensemble"] = metrics_test

    # Print and save results
    for m in metricsDict:
        # The average metrics for training dataset
        #prettyPrint("Metrics using %s-fold cross validation and %s" % (arguments.kfold, m), "output")
        prettyPrint("Accuracy: %s" % str(metricsDict[m]["accuracy"]), "output")
        prettyPrint("Recall: %s" % str(metricsDict[m]["recall"]), "output")
        prettyPrint("Specificity: %s" % str(metricsDict[m]["specificity"]), "output")
        prettyPrint("Precision: %s" % str(metricsDict[m]["precision"]), "output")
        prettyPrint("F1 Score: %s" % str(metricsDict[m]["f1score"]), "output")

     # Print and save results [FOR THE TEST DATASET]
    for m in metricsDict_test:
        # The average metrics for training dataset
        #prettyPrint("Metrics using %s-fold cross validation and %s" % (arguments.kfold, m), "output")
        prettyPrint("Accuracy: %s" % str(metricsDict_test[m]["accuracy"]), "output")
        prettyPrint("Recall: %s" % str(metricsDict_test[m]["recall"]), "output")
        prettyPrint("Specificity: %s" % str(metricsDict_test[m]["specificity"]), "output")
        prettyPrint("Precision: %s" % str(metricsDict_test[m]["precision"]), "output")
        prettyPrint("F1 Score: %s" % str(metricsDict_test[m]["f1score"]), "output")


def generate_labeled_list(path, label):
    l = []
    for feature_file in glob.iglob(path + '/**/*.tfidf'):
        n = np.asarray(eval(open(feature_file).read()))
        l.append((n, label))
    return l


if __name__ == "__main__":
    main()

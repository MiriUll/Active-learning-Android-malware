import argparse
import json
import matplotlib.pyplot as plt
from cycler import cycler

########################
# Plot results as file #
########################

# define a cylcer for plotting
plt.rc('axes', prop_cycle=(cycler('color', ['r', 'g', 'b', 'y']) +
                           cycler('linestyle', ['-', '-.', '--', ':'])))

parser = argparse.ArgumentParser(prog="compare_results.py",
                                 description="Visualize results from training")
parser.add_argument("-r", "--result", help="path to result json", required=True)
parser.add_argument("-c", "--compare", help="path to compare result", nargs='+', required=False, default='inactive')
parser.add_argument("-m", "--metric", help="the metric to print", required=False, default="accuracy",
                    choices={'accuracy', 'f1score', 'precision', 'recall', 'specificity', 'mcc'})
parser.add_argument("-t", "--training", help="plot training or test results", required=False, default="training",
                    choices=["training", "test"])
parser.add_argument("-a", "--active", help="Compare active or inactive metrics", required=False, default="active")
parser.set_defaults(cross_val=False)
parser.add_argument("-cv", "-cross_val", help="evaluate cross val scores", required=False, action="store_true", dest='cross_val')
arguments = parser.parse_args()


def metrics_to_list(result):
    """
    get a list of a specific metrics in all iterations
    :param result: result dict
    :return: list of metric
    """
    sub_dic = result[arguments.active][arguments.training]
    metrics = []
    for res in sorted(sub_dic, key=int):
        metrics.append(sub_dic[res][arguments.metric])
    return metrics


def find_argument_difference(result, compare):
    """
    find different arguments of result and compare
    :param result: result dict
    :param compare: result dict to compare
    :return: differences as dict
    """
    diff = []
    for a in result['arguments']:
        if a == 'training: num benign' or a == 'training: num malicious' or a == 'test_data' or a == 'training_data':
            continue
        if not result['arguments'][a] == compare['arguments'][a]:
            diff.append(a)
    return diff


def label_from_diff_arguments(result, different_arguments):
    """
    find differences to label subplots
    :param result: result dict
    :param different_arguments: different arguments of result and compare
    :return: label
    """
    res_label = ""
    for arg in different_arguments:
        res_label += arg + ": " + str(result['arguments'][arg]) + ' '
    return res_label


def plot_incative(result, label='', color='b', line='--'):
    """
    plot incative baseline
    :param result: result dict
    :param label: label of incative line
    :param color: color or inactive
    :param line: linestyle of inactive
    :return:
    """
    inact = result['inactive'][arguments.training][arguments.metric]
    ax = plt.subplot()
    ax.axhline(y=inact, color=color, linestyle=line, label='inactive' + ' ' + label)
    #ax.axhline(y=inact, color=color, linestyle=line, label='inactive')
    ax.text(0, inact, round(inact, 4), color=color, transform=ax.get_yaxis_transform(),
            ha="right", va="center")


def plot_result():
    """
    plot results as lineplots per iteration
    """
    # load results dict and generate metric list
    result = json.load(open(arguments.result))
    metrics = metrics_to_list(result)

    if arguments.compare == 'inactive':
        # compare active and inactive classifier of same experiment
        plt.plot(metrics, label='active')
        plot_incative(result, color='g')

        ymax = max(metrics)
        ypos_max = metrics.index(ymax)
        ymin = min(metrics)
        ypos_min = metrics.index(ymin)
        plt.annotate(str(ypos_max + 1) + ': ' + str(round(ymax, 4)), xy=(ypos_max, ymax), xytext=(ypos_max, ymax + 0.1),
                     arrowprops=dict(facecolor='black', arrowstyle='->'))
        plt.annotate(str(ypos_min + 1) + ': ' + str(round(ymin, 4)), xy=(ypos_min, ymin), xytext=(ypos_min, ymin + 0.1),
                     arrowprops=dict(facecolor='black', arrowstyle='->'))

        save = arguments.result.rsplit('.', 1)[0] + '-' + arguments.training + '-' + arguments.metric + '.pdf'
    else:
        # compare different experiments
        compare = json.load(open(arguments.compare[0]))
        different_arguments = find_argument_difference(result, compare)
        res_label = label_from_diff_arguments(result, different_arguments)
        plt.plot(metrics, label=res_label)
        plot_incative(result, res_label, color='b', line='--')
        for comp in arguments.compare:
            compare = json.load(open(comp))
            metrics_comp = metrics_to_list(compare)
            comp_label = label_from_diff_arguments(compare, different_arguments)
            plot_incative(compare, comp_label, 'c', ':')
            plt.plot(metrics_comp, label=comp_label)
        save = 'compare_' + ''.join(different_arguments) + '.pdf'

    plt.xlabel('Iteration')
    plt.ylabel(arguments.training + ' ' + arguments.metric)
    #plt.ylim(-0.3, 0.3)
    #plt.ylim(-0.1, 1.1)
    if len(metrics) < 10:
        plt.xticks(range(len(metrics)))

    plt.legend()
    plt.savefig(save, format='pdf')
    plt.show()


def validate_param():
    """
    plot validation scores as boxplots
    """
    result = json.load(open(arguments.result))
    compare = json.load(open(arguments.compare[0]))
    different_arguments = find_argument_difference(result, compare)
    res_label = label_from_diff_arguments(result, different_arguments)
    mean = round(sum(result['inactive']['training']['cross_val'])/ len(result['inactive']['training']['cross_val']), 4)
    plt.annotate(mean, xy=(1, mean), xytext=(1 - 0.2, mean - 0.0004),
                 arrowprops=dict(facecolor='black', arrowstyle='->'))
    data = [result['inactive']['training']['cross_val']]
    labels = [res_label]
    for comp in arguments.compare:
        compare = json.load(open(comp))
        # metrics_comp = metrics_to_list(compare)
        comp_label = label_from_diff_arguments(compare, different_arguments)
        data.append(compare['inactive']['training']['cross_val'])
        labels.append(comp_label)
        mean = round(sum(compare['inactive']['training']['cross_val']) / len(compare['inactive']['training']['cross_val']), 4)
        plt.annotate(mean, xy=(len(labels), mean), xytext=(len(labels) - 0.2, mean - 0.0004),
                     arrowprops=dict(facecolor='black', arrowstyle='->'))
    plt.boxplot(data, labels=labels, meanline=True, showmeans=True, medianprops={'color': 'r'})
    save = 'cross_val_' + ''.join(different_arguments) + '.pdf'
    plt.ylabel('Disrtibution of cross val scores')
    #plt.xticks(labels)

    plt.legend()
    plt.savefig(save, format='pdf')
    plt.show()


if __name__ == '__main__':
    if arguments.cross_val:
        validate_param()
    else:
        plot_result()

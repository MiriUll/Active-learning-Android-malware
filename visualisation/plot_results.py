import argparse
import json
import glob
import matplotlib.pyplot as plt
from cycler import cycler

plt.rc('axes', prop_cycle=(cycler('color', ['r', 'g', 'b', 'y']) +
                           cycler('linestyle', ['-', '--', ':', '-.'])))

parser = argparse.ArgumentParser(prog="compare_results.py",
                                 description="Visualize results from training")
parser.add_argument("-r", "--result", help="path to result json", required=True)
parser.add_argument("-c", "--compare", help="path to compare result", nargs='+', required=False, default='inactive')
parser.add_argument("-m", "--metric", help="the metric to print", required=False, default="accuracy",
                    choices={'accuracy', 'f1score', 'precision', 'recall', 'specificity', 'mcc'})
parser.add_argument("-t", "--training", help="plot training or test results", required=False, default="training",
                    choices=["training", "test"])
parser.add_argument("-a", "--active", help="Compare active or inactive metrics", required=False, default="active")
parser.set_defaults(cross_val=False)
parser.add_argument("-cv", "-cross_val", help="evaluate cross val scores", required=False, action="store_true", dest='cross_val')
arguments = parser.parse_args()


def metrics_to_list(result):
    sub_dic = result[arguments.active][arguments.training]
    metrics = []
    for res in sub_dic:
        metrics.append(sub_dic[res][arguments.metric])
    return metrics


def find_argument_difference(result, compare):
    diff = []
    for a in result['arguments']:
        if a == 'training: num benign' or a == 'training: num malicious' or a == 'test_data' or a == 'training_data':
            continue
        if not result['arguments'][a] == compare['arguments'][a]:
            diff.append(a)
    return diff


def label_from_diff_arguments(result, different_arguments):
    res_label = ""
    for arg in different_arguments:
        res_label += arg + ": " + str(result['arguments'][arg]) + ' '
    return res_label


def plot_result():
    result = json.load(open(arguments.result))

    metrics = metrics_to_list(result)

    if arguments.compare == 'inactive':
        plt.plot(metrics, label='active')
        plt.axhline(y=result['inactive'][arguments.training][arguments.metric], color='g', linestyle='--', label='inactive')

        ymax = max(metrics)
        ypos_max = metrics.index(ymax)
        ymin = min(metrics)
        ypos_min = metrics.index(ymin)
        plt.annotate(ymax, xy=(ypos_max, ymax), xytext=(ypos_max, ymax + 0.1),
                     arrowprops=dict(facecolor='black', arrowstyle='->'))
        plt.annotate(ymin, xy=(ypos_min, ymin), xytext=(ypos_min, ymin - 0.1),
                     arrowprops=dict(facecolor='black', arrowstyle='->'))

        save = arguments.result.rsplit('.', 1)[0] + '-' + arguments.training + '-' + arguments.metric + '.png'
    else:
        compare = json.load(open(arguments.compare[0]))
        different_arguments = find_argument_difference(result, compare)
        res_label = label_from_diff_arguments(result, different_arguments)
        plt.plot(metrics, label=res_label)
        for comp in arguments.compare:
            compare = json.load(open(comp))
            metrics_comp = metrics_to_list(compare)
            comp_label = label_from_diff_arguments(compare, different_arguments)
            plt.plot(metrics_comp, label=comp_label)
        save = 'compare_' + ''.join(different_arguments) + '.png'

    plt.xlabel('Iteration')
    plt.ylabel(arguments.training + ' ' + arguments.metric)
    plt.ylim(-0.1, 1.1)
    if len(metrics) < 10:
        plt.xticks(range(len(metrics)))

    plt.legend()
    plt.savefig(save)
    plt.show()


def validate_param():
    result = json.load(open(arguments.result))
    compare = json.load(open(arguments.compare[0]))
    different_arguments = find_argument_difference(result, compare)
    res_label = label_from_diff_arguments(result, different_arguments)
    mean = round(sum(result['inactive']['training']['cross_val'])/ len(result['inactive']['training']['cross_val']), 4)
    plt.annotate(mean, xy=(1, mean), xytext=(1 - 0.2, mean + 0.01),
                 arrowprops=dict(facecolor='black', arrowstyle='->'))
    data = [result['inactive']['training']['cross_val']]
    labels = [res_label]
    for comp in arguments.compare:
        compare = json.load(open(comp))
        # metrics_comp = metrics_to_list(compare)
        comp_label = label_from_diff_arguments(compare, different_arguments)
        data.append(compare['inactive']['training']['cross_val'])
        labels.append(comp_label)
        mean = round(sum(compare['inactive']['training']['cross_val']) / len(compare['inactive']['training']['cross_val']), 4)
        plt.annotate(mean, xy=(len(labels), mean), xytext=(len(labels) - 0.2, mean + 0.01),
                     arrowprops=dict(facecolor='black', arrowstyle='->'))
    plt.boxplot(data, labels=labels, meanline=True, showmeans=True, medianprops={'color': 'r'})
    save = 'cross_val_' + ''.join(different_arguments) + '.png'
    plt.ylabel('Disrtibution of cross val scores')
    #plt.xticks(labels)

    plt.legend()
    plt.savefig(save)
    plt.show()


if __name__ == '__main__':
    if arguments.cross_val:
        validate_param()
    else:
        plot_result()

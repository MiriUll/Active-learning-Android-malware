import argparse
from utils.config import *
from utils.utils import *
from utils.alp.active_learning import ActiveLearner
from utils.learners import predictAndTestNB, calculateMetrics_multiclass
import sys
import time
sys.path.insert(0, aion_path)
import Aion
from Aion.conf.config import *

Aion.conf.config.LOG_FILE = aion_log_file
from Aion.data_inference.learning import ScikitLearners
from Aion.utils.db import *

percentage_test = 0.2
percentage_unlabeled = 0.3
percentage_prediction_per_iteration = 0.5

#training_data, test_data, y_train, y_test = load_datasets(malware_features, goodware_features, percentage_test)
training_data, test_data, y_train, y_test = load_datasets_ordered()
prettyPrint("Retrieved %s apps for training" % len(training_data))
print(y_train.count(0), y_train.count(1), y_test.count(0), y_test.count(1))
training_label, training_unlabeld, y_label, y_oracle = split_dataset(training_data, y_train, percentage_unlabeled)


def next_training_data(AL, clf):
    """
    get next training data based on active learning ranking
    :param AL: active learner
    :param clf: probabilistic classifier
    :return: training data and corresponding label
    """
    ranking = AL.rank(clf, np.asarray(training_unlabeld), num_queries=percentage_prediction_per_iteration)
    ranking.sort()
    ranking = ranking[::-1]
    new_training_data, new_labels = [], []
    for index in ranking:
        new_training_data.append(training_unlabeld.pop(index))
        new_labels.append(y_oracle.pop(index))
    return new_training_data, new_labels


def train_naive_bayes(query_strategy, accuracymargin, maxiterations):
    prettyPrint("Welcome to the \"Aion\"'s naive active experiment with naive bayes")

    iteration = 1  # Initial values
    dummy_metrics = {"accuracy": 0.0, "recall": 0.0, "specificity": 0.0, "precision": 0.0, "f1score": 0.0}
    metricsDict, metricsDict_test = {}, {}
    metricsDict[0] = dummy_metrics
    metricsDict[-1] = dummy_metrics

    AL = ActiveLearner(strategy=query_strategy)

    Xtr, ytr = training_label, y_label
    Xte, yte = test_data, y_test

    while (round(metricsDict[iteration-1]["f1score"] - metricsDict[iteration-2]["f1score"], 2) >=
           -(float(accuracymargin) / 100.0)) and (iteration <= int(maxiterations) and len(training_unlabeld) > 0):

        if iteration > 1:
            # Add suggested data to training data
            new_training_data, new_label = next_training_data(AL, clf)
            Xtr += new_training_data
            ytr += new_label

        ####################################
        # Train naive bayes classifier #
        ###################################
        prettyPrint("Classifying using Multinomial Naive Bayes")
        clf, predicted, predicted_test = predictAndTestNB(Xtr, ytr, Xte, yte)
        metrics = calculateMetrics_multiclass(ytr, predicted)
        metrics_test = calculateMetrics_multiclass(yte, predicted_test)
        metricsDict[iteration] = metrics
        metricsDict_test[iteration] = metrics_test

        # Update the iteration number
        iteration += 1

    # Final Results
    del metricsDict[-1]
    prettyPrint("Training results after %s iterations" % str(iteration - 1), "output")
    print_metrics(metricsDict)

    prettyPrint("Test metrics")
    print_metrics(metricsDict_test)

    timestamp = str(int(time.time()))
    save_metrics_to_file(results_folder + 'naive_bayes_' + timestamp + '.json', metricsDict)
    save_metrics_to_file(results_folder + 'naive_bayes_test_' + timestamp + '.json', metricsDict_test)


def train(query_strategy_single, query_strategy_ensemble, accuracymargin, maxiterations):
    """
    train classifier in certain iterations and test
    :param query_strategy_single: al query strategy for single classifier
    :param query_strategy_ensemble: al query strategy for ensemble
    :param accuracymargin: maximum change in accuracy per iteration
    :param maxiterations: maximum iterations
    """
    prettyPrint("Welcome to the \"Aion\"'s naive active experiment")

    iteration = 1  # Initial values
    allMetrics = {}
    allMetrics_test = {}
    currentMetrics = {"accuracy": 0.0, "recall": 0.0, "specificity": 0.0, "precision": 0.0, "f1score": 0.0}
    previousMetrics = {"accuracy": -1.0, "recall": -1.0, "specificity": -1.0, "precision": -1.0, "f1score": -1.0}
    al_clf = None

    AL = ActiveLearner(strategy=query_strategy_single)
    AL_ensemlbe = ActiveLearner(strategy=query_strategy_ensemble)

    Xtr, ytr = training_label, y_label
    Xte, yte = test_data, y_test

    while (round(currentMetrics["f1score"] - previousMetrics["f1score"], 2) >=
           -(float(accuracymargin) / 100.0)) and (iteration <= int(maxiterations) and
                                                  percentage_prediction_per_iteration * len(training_unlabeld) >= 1):
        metricsDict, metricsDict_test = {}, {}
        ####################################
        # Ensemble of learning algorithms #
        ###################################
        prettyPrint("Ensemble mode classification: K-NN, SVM, and Random Forests")
        predicted_trees100 = []
        # Classifying using K-nearest neighbors
        K = [10, 25, 50, 100, 250, 500]
        #K = []
        for k in K:
            prettyPrint("Classifying using K-nearest neighbors with K=%s" % k)
            clf, predicted, predicted_test = ScikitLearners.predictAndTestKNN(Xtr, ytr, Xte, yte)
            metrics = ScikitLearners.calculateMetrics(ytr, predicted)
            metrics_test = ScikitLearners.calculateMetrics(yte, predicted_test)
            metricsDict["KNN%s" % k] = metrics
            metricsDict_test["KNN%s" % k] = metrics_test

        # Classifying using Random Forests
        E = [10, 25, 50, 75, 100]
        for e in E:
            prettyPrint("Classifying using Random Forests with %s estimators" % e)
            clf, predicted, predicted_test = ScikitLearners.predictAndTestRandomForest(Xtr, ytr, Xte, yte,
                                                                                       estimators=e)
            metrics = ScikitLearners.calculateMetrics(ytr, predicted)
            metrics_test = ScikitLearners.calculateMetrics(yte, predicted_test)
            metricsDict["Trees%s" % e] = metrics
            metricsDict_test["Trees%s" % e] = metrics_test
        al_clf = clf

        # Classifying using SVM
        prettyPrint("Classifying using Support vector machines")
        clf, predicted, predicted_test = ScikitLearners.predictAndTestSVM(Xtr, ytr, Xte, yte)
        metrics = ScikitLearners.calculateMetrics(ytr, predicted)
        metrics_test = ScikitLearners.calculateMetrics(yte, predicted_test)
        metricsDict["SVM"] = metrics
        metricsDict_test["SVM"] = metrics_test

        # Now do the majority voting ensemble
        allCs = ["KNN-%s" % x for x in K] + ["FOREST-%s" % e for e in E] + ["SVM"]
        clf, predicted, predicted_test = ScikitLearners.predictAndTestEnsemble(Xtr, ytr, Xte, yte,
                                                                               classifiers=allCs)
        metrics = ScikitLearners.calculateMetrics(predicted, ytr)  # Used to decide upon whether to iterate more
        metrics_test = ScikitLearners.calculateMetrics(predicted_test, yte)
        metricsDict["Ensemble"] = metrics
        metricsDict_test["Ensemble"] = metrics_test

        # Print and save results
        # print_metrics(metricsDict)
        allMetrics[iteration] = metricsDict

        # Add suggested data to training data
        new_training_data, new_label = next_training_data(AL, al_clf)
        Xtr += new_training_data
        ytr += new_label

        # Swapping metrics
        previousMetrics = currentMetrics
        currentMetrics = metricsDict["Ensemble"]

        # Print and save results [FOR THE TEST DATASET]
        # print_metrics(metricsDict_test)
        allMetrics_test[iteration] = metricsDict_test

        # Update the iteration number
        iteration += 1

    # Final Results
    prettyPrint("Training results after %s iterations" % str(iteration - 1), "output")
    prettyPrint("Accuracy: %s" % currentMetrics["accuracy"], "output")
    prettyPrint("Recall: %s" % currentMetrics["recall"], "output")
    prettyPrint("Specificity: %s" % currentMetrics["specificity"], "output")
    prettyPrint("Precision: %s" % currentMetrics["precision"], "output")
    prettyPrint("F1 Score: %s" % currentMetrics["f1score"], "output")

    prettyPrint("Test metrics")
    print_metrics(metricsDict_test)

    timestamp = str(int(time.time()))
    save_metrics_to_file(results_folder + 'naive_active_' + timestamp + '.json', allMetrics)
    save_metrics_to_file(results_folder + 'naive_active_test_' + timestamp + '.json', allMetrics_test)


def print_metrics(metricsDict):
    for m in metricsDict:
        # The average metrics for training dataset
        prettyPrint("Metrics using" + str(m), "output")
        prettyPrint("Accuracy: %s" % str(metricsDict[m]["accuracy"]), "output")
        prettyPrint("Recall: %s" % str(metricsDict[m]["recall"]), "output")
        prettyPrint("Specificity: %s" % str(metricsDict[m]["specificity"]), "output")
        prettyPrint("Precision: %s" % str(metricsDict[m]["precision"]), "output")
        prettyPrint("F1 Score: %s" % str(metricsDict[m]["f1score"]), "output")


def defineArguments():
    parser = argparse.ArgumentParser(prog="naive_active_training.py",
                                     description="Train malware detection with active learning")
    parser.add_argument("-q", "--query_strategy", help="Query strategy for active learner",
                        required=False, default="entropy")
    parser.add_argument("-a", "--alpha", help="Parameter alpha for naive bayes", required=False, default=1.0)
    parser.add_argument("-i", "--iterations", help="Maximum number of iterations", required=False, default=5)
    return parser


if __name__ == '__main__':
    #train('entropy', 'vote_entropy', 0.2, 5)
    argumentParser = defineArguments()
    arguments = argumentParser.parse_args()
    train_naive_bayes(arguments.query_strategy, arguments.alpha, arguments.iterations)

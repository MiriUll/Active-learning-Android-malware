import argparse
from utils.config import *
from utils.utils import *
from utils.alp.active_learning import ActiveLearner
from utils.learners import calculateMetrics_multiclass, kFoldNB, kFoldDT
import sys
from gensim.corpora import dictionary

sys.path.insert(0, aion_path)
import Aion
from Aion.conf.config import *
from sklearn import tree

Aion.conf.config.LOG_FILE = aion_log_file
from Aion.utils.db import *
from Aion.data_inference.learning import ScikitLearners


def cast_type_of_argument(arg):
    try:
        return int(arg)
    except ValueError:
        return float(arg)


def defineArguments():
    parser = argparse.ArgumentParser(prog="naive_active_training.py",
                                     description="Train malware detection with active learning")
    parser.add_argument("-q", "--query_strategy", help="Query strategy for active learner",
                        required=False, default="entropy",
                        choices=['entropy', 'max_margin', 'least_confident', 'random'])
    parser.add_argument("-a", "--alpha", help="Parameter alpha for naive bayes", required=False, default=1.0,
                        type=float)
    parser.add_argument("-i", "--iterations", help="Maximum number of iterations", required=False, default=10, type=int)
    parser.add_argument("-acc", "--accuracymargin", help="accuracy margin for iterations", required=False, default=1,
                        type=float)
    parser.set_defaults(compare_virus_total=True, ignore_ambiguous=True, tfidf_features=True, nb=True)
    parser.add_argument("-v", "--compare_virus_total", help="disable virus total comparation", required=False,
                        default=True, dest='compare_virus_total', action='store_false')
    parser.add_argument("-ig", "--ignore_ambiguous", help="analyse ambiguous vt reports", required=False, default=True,
                        dest='compare_virus_total', action='store_false')
    parser.add_argument("-e", "--engineered", help="Use engineered features instead of tfidf", required=False,
                        dest='tfidf_features', action='store_false')
    parser.add_argument("-n", "--not_nb", help="use different clf", dest="nb", action="store_false")
    parser.add_argument("-t", "--test_percentage", help="percentage of data that is test data", required=False,
                        default=0.2, type=float)
    parser.add_argument("-u", "--unlabeled_percentage", help="percentage of unlabeled data", required=False,
                        default=0.6, type=float)
    parser.add_argument("-ppi", "--prediction_per_iteration",
                        help="percentage of additional training data per iteration",
                        required=False, default=0.1, type=cast_type_of_argument)
    parser.add_argument("-m", "--mode", help="Choose training mode", required=False,
                        choices=['active', 'traditional', 'both'], default='both')
    parser.add_argument("-s", "--labeling_schema", help="labeling schema for vt labels", required=False, default="zero",
                        choices=["zero", "ambiguous", "fifty_percent"])
    parser.add_argument("-k", "--kfold", help="Kfold value k", required=False, default=5, type=int)
    parser.add_argument("-b", "--balance", help="factor to balance amd and gplay apps", default=1.0, type=float,
                        required=False)
    return parser


argumentParser = defineArguments()
arguments = argumentParser.parse_args()
if not arguments.compare_virus_total:
    arguments.labeling_schema = "original"

#training_data, test_data, y_train, y_test, dataset_training, dataset_test = load_datasets(malware_features, goodware_features, arguments.test_percentage,labeling_schema=arguments.labeling_schema, compare_vt=arguments.compare_virus_total,
#        ignore_ambiguous=arguments.ignore_ambiguous)
# training_data, test_data, y_train, y_test, dataset_training, dataset_test = load_datasets_for_validation(balance_factor=arguments.balance,
# training_data, test_data, y_train, y_test, dataset_training, dataset_test = load_datasets_ordered_swap_training(balance_factor=arguments.balance,
if arguments.tfidf_features:
    training_data, test_data, y_train, y_test, dataset_training, dataset_test = load_datasets_ordered(
        labeling_schema=arguments.labeling_schema, compare_vt=arguments.compare_virus_total,
        ignore_ambiguous=arguments.ignore_ambiguous)
else:
    training_data, test_data, y_train, y_test, dataset_training, dataset_test = load_datasets_engineered_features(
        labeling_schema=arguments.labeling_schema, compare_vt=arguments.compare_virus_total,
        ignore_ambiguous=arguments.ignore_ambiguous)
prettyPrint("Retrieved %s apps for training" % len(training_data))
print(y_train.count(0), y_train.count(1), y_test.count(0), y_test.count(1))
training_label, training_unlabeld, y_label, y_oracle = split_dataset(training_data, y_train,
                                                                     arguments.unlabeled_percentage)
prettyPrint("Start training with %s labeled data" % len(training_label))


def next_training_data(AL, clf):
    """
    get next training data based on active learning ranking
    :param AL: active learner
    :param clf: probabilistic classifier4
    :return: training data and corresponding label
    """
    ranking = AL.rank(clf, np.asarray(training_unlabeld), num_queries=arguments.prediction_per_iteration)
    ranking.sort()
    ranking = ranking[::-1]
    new_training_data, new_labels = [], []
    for index in ranking:
        new_training_data.append(training_unlabeld.pop(index))
        new_labels.append(y_oracle.pop(index))
    prettyPrint("Adding %s samples to training data" % len(new_training_data))
    return new_training_data, new_labels


def train_naive_bayes():
    prettyPrint("Welcome to the \"Aion\"'s naive active experiment with naive bayes")

    iteration = 1  # Initial values
    dummy_metrics = {"accuracy": 0.0, "recall": 0.0, "specificity": 0.0, "precision": 0.0, "f1score": 0.0,
                     "cross_val": 0.0}
    metrics_dict, metricsDict_test = {}, {}
    metrics_dict[0] = dummy_metrics
    metrics_dict[-1] = dummy_metrics

    AL = ActiveLearner(strategy=arguments.query_strategy)

    Xtr, ytr = training_label, y_label
    Xte, yte = test_data, y_test

    while (round(metrics_dict[iteration - 1]["f1score"] - metrics_dict[iteration - 2]["f1score"], 2) >=
           -(arguments.accuracymargin / 100.0)) and \
            (iteration <= int(arguments.iterations) and len(training_unlabeld) > 0):

        if iteration > 1:
            # Add suggested data to training data
            new_training_data, new_label = next_training_data(AL, clf)
            Xtr += new_training_data
            ytr += new_label

        ####################################
        # Train naive bayes classifier #
        ###################################
        if arguments.nb:
            prettyPrint("Classifying using Multinomial Naive Bayes")
            clf, predicted, predicted_test, cv_scores = kFoldNB(arguments.kfold, Xtr, ytr, Xte, yte, alpha=arguments.alpha)
        else:
            '''cv_scores = []
            clf, predicted, predicted_test = ScikitLearners.predictAndTestRandomForest(Xtr, ytr, Xte, yte,
                                                                                       estimators=100,
                                                                                       selectKBest=0)'''
            clf, predicted, predicted_test, cv_scores = kFoldDT(arguments.kfold, Xtr, ytr, Xte, yte)
        metrics = calculateMetrics_multiclass(ytr, predicted)
        metrics["cross_val"] = cv_scores
        metrics_test = calculateMetrics_multiclass(yte, predicted_test)
        metrics_dict[iteration] = metrics
        metricsDict_test[iteration] = metrics_test

        prettyPrint("F1-score in iteration %s" % str(iteration) + ": %s" % str(metrics_dict[iteration]["f1score"]))

        # Update the iteration number
        iteration += 1

    # Final Results
    del metrics_dict[-1]
    del metrics_dict[0]
    prettyPrint("Training results after %s iterations" % str(iteration - 1), "output")
    print_metrics(metrics_dict)

    prettyPrint("Test metrics")
    print_metrics(metricsDict_test)

    resultsDict = {'training': metrics_dict, "test": metricsDict_test}

    if arguments.nb:
        prettyPrint("Print 10 best feature for malware")
        mal_prob_sorted = clf.feature_log_prob_[0, :].argsort()
        find_best_features(mal_prob_sorted)
        prettyPrint("Print 10 best features for goodware")
        good_prob_sorted = clf.feature_log_prob_[1, :].argsort()
        find_best_features(good_prob_sorted)

    return resultsDict


def find_best_features(feature_ranking):
    id2token_dictionary = dictionary.Dictionary.load_from_text(corpus_path)
    tokenTuples = [tuple(map(int, i.split(' '))) for i in open(feature_mapping_path)]
    for i in range(10):
        featureIndex = feature_ranking[i]
        tokenKey = tokenTuples[featureIndex][0]
        print(featureIndex, tokenKey, id2token_dictionary[tokenKey])


def train_inactive_NB():
    prettyPrint("Welcome to the \"Aion\"'s naive inactive experiment with naive bayes")

    metricsDict = {}
    Xtr, ytr = training_data, y_train
    Xte, yte = test_data, y_test

    ####################################
    # Train naive bayes classifier #
    ###################################
    if arguments.nb:
        prettyPrint("Classifying using Multinomial Naive Bayes")
        clf, predicted, predicted_test, cv_scores = kFoldNB(arguments.kfold, Xtr, ytr, Xte, yte, alpha=arguments.alpha)
    else:
        '''cv_scores = []
        clf, predicted, predicted_test = ScikitLearners.predictAndTestRandomForest(Xtr, ytr, Xte, yte,
                                                                               estimators=100,
                                                                               selectKBest=0)'''
        estimator, predicted, predicted_test, cv_scores = kFoldDT(arguments.kfold, Xtr, ytr, Xte, yte)
        n_nodes = estimator.tree_.node_count
        children_left = estimator.tree_.children_left
        children_right = estimator.tree_.children_right
        feature = estimator.tree_.feature
        threshold = estimator.tree_.threshold
        value = estimator.tree_.value
        print(value)

        # The tree structure can be traversed to compute various properties such
        # as the depth of each node and whether or not it is a leaf.
        node_depth = np.zeros(shape=n_nodes, dtype=np.int64)
        is_leaves = np.zeros(shape=n_nodes, dtype=bool)
        stack = [(0, -1)]  # seed is the root node id and its parent depth
        while len(stack) > 0:
            node_id, parent_depth = stack.pop()
            node_depth[node_id] = parent_depth + 1

            # If we have a test node
            if (children_left[node_id] != children_right[node_id]):
                stack.append((children_left[node_id], parent_depth + 1))
                stack.append((children_right[node_id], parent_depth + 1))
            else:
                is_leaves[node_id] = True

        print("The binary tree structure has %s nodes and has "
              "the following tree structure:"
              % n_nodes)
        for i in range(n_nodes):
            if is_leaves[i]:
                print("%snode=%s leaf node -> %s." % (node_depth[i] * "\t", i, np.argmax(value[i][0])))
            else:
                print("%snode=%s test node: go to node %s if X[:, %s] <= %s else to "
                      "node %s."
                      % (node_depth[i] * "\t",
                         i,
                         children_left[i],
                         feature[i],
                         threshold[i],
                         children_right[i],
                         ))
        print()

        # First let's retrieve the decision path of each sample. The decision_path
        # method allows to retrieve the node indicator functions. A non zero element of
        # indicator matrix at the position (i, j) indicates that the sample i goes
        # through the node j.

        node_indicator = estimator.decision_path(Xte)

        # Similarly, we can also have the leaves ids reached by each sample.

        leave_id = estimator.apply(Xte)

        # Now, it's possible to get the tests that were used to predict a sample or
        # a group of samples. First, let's make it for the sample.

        sample_id = 0
        node_index = node_indicator.indices[node_indicator.indptr[sample_id]:
                                            node_indicator.indptr[sample_id + 1]]

        print('Rules used to predict sample %s: ' % sample_id)
        print(node_index)
        for node_id in node_index:
	    print(node_id)
            if leave_id[sample_id] == node_id:
                continue
            if (Xte[sample_id][feature[node_id]] <= threshold[node_id]):
                threshold_sign = "<="
            else:
                threshold_sign = ">"

            print("decision id node %s : (Xte[%s, %s] (= %s) %s %s)"
                  % (node_id,
                     sample_id,
                     feature[node_id],
                     Xte[sample_id][ feature[node_id]],
                     threshold_sign,
                     threshold[node_id]))

        # For a group of samples, we have the following common node.
        sample_ids = [0, 1]
        common_nodes = (node_indicator.toarray()[sample_ids].sum(axis=0) ==
                        len(sample_ids))

        common_node_id = np.arange(n_nodes)[common_nodes]

        print("\nThe following samples %s share the node %s in the tree"
              % (sample_ids, common_node_id))
        print("It is %s %% of all nodes." % (100 * len(common_node_id) / n_nodes,))
    metrics = calculateMetrics_multiclass(ytr, predicted)
    metrics_test = calculateMetrics_multiclass(yte, predicted_test)
    metricsDict['training'] = metrics
    metricsDict['training']["cross_val"] = cv_scores
    metricsDict['test'] = metrics_test

    # Final Results
    prettyPrint("Training results")
    print_metrics(metricsDict)

    prettyPrint("Test metrics")
    print_metrics({'test': metricsDict['test']})

    return metricsDict


def print_metrics(metrics_dict):
    for m in metrics_dict:
        # The average metrics for training dataset
        prettyPrint("Metrics using " + str(m), "output")
        prettyPrint("Accuracy: %s" % str(metrics_dict[m]["accuracy"]), "output")
        prettyPrint("Recall: %s" % str(metrics_dict[m]["recall"]), "output")
        prettyPrint("Specificity: %s" % str(metrics_dict[m]["specificity"]), "output")
        prettyPrint("Precision: %s" % str(metrics_dict[m]["precision"]), "output")
        prettyPrint("F1 Score: %s" % str(metrics_dict[m]["f1score"]), "output")
        if "cross_val" in metrics_dict[m]:
            prettyPrint("Cross validation scores: %s" % str(metrics_dict[m]["cross_val"]), "output")


def non_default_arguments():
    argument_string = ''
    for arg in vars(arguments):
        if getattr(arguments, arg) != argumentParser.get_default(arg):
            argument_string += '-' + arg.capitalize() + str(getattr(arguments, arg)).capitalize()
    return argument_string


if __name__ == '__main__':
    arg_string = non_default_arguments()
    metricsDict = {'arguments': vars(arguments)}
    metricsDict['arguments']["training_data"] = dataset_training
    metricsDict['arguments']["test_data"] = dataset_test
    metricsDict['arguments']['training: num benign'] = y_train.count(0)
    metricsDict['arguments']['training: num malicious'] = y_train.count(1)
    if arguments.mode is 'active':
        metricsDict["active"] = train_naive_bayes()
    elif arguments.mode is 'traditional':
        metricsDict["inactive"] = train_inactive_NB()
    else:
        metricsDict["active"] = train_naive_bayes()
        metricsDict["inactive"] = train_inactive_NB()
    storename = 'naive_bayes' if arguments.nb else 'decision_tree'
    save_metrics_to_file(results_folder + storename + arg_string + '.json', metricsDict)

import argparse
from utils.config import *
from utils.utils import *
from utils.alp.active_learning import ActiveLearner
from utils.learners import calculateMetrics_multiclass, kFoldNB
import sys
from gensim.corpora import dictionary

sys.path.insert(0, aion_path)
import Aion
from Aion.conf.config import *

Aion.conf.config.LOG_FILE = aion_log_file
from Aion.utils.db import *
from Aion.data_inference.learning import ScikitLearners


def cast_type_of_argument(arg):
    try:
        return int(arg)
    except ValueError:
        return float(arg)


def defineArguments():
    parser = argparse.ArgumentParser(prog="naive_active_training.py",
                                     description="Train malware detection with active learning")
    parser.add_argument("-q", "--query_strategy", help="Query strategy for active learner",
                        required=False, default="entropy",
                        choices=['entropy', 'max_margin', 'least_confident', 'random'])
    parser.add_argument("-a", "--alpha", help="Parameter alpha for naive bayes", required=False, default=1.0,
                        type=float)
    parser.add_argument("-i", "--iterations", help="Maximum number of iterations", required=False, default=10, type=int)
    parser.add_argument("-acc", "--accuracymargin", help="accuracy margin for iterations", required=False, default=1,
                        type=float)
    parser.set_defaults(compare_virus_total=True, ignore_ambiguous=True, tfidf_features=True)
    parser.add_argument("-v", "--compare_virus_total", help="disable virus total comparation", required=False,
                        default=True, dest='compare_virus_total', action='store_false')
    parser.add_argument("-ig", "--ignore_ambiguous", help="analyse ambiguous vt reports", required=False, default=True,
                        dest='compare_virus_total', action='store_false')
    parser.add_argument("-e", "--engineered", help="Use engineered features instead of tfidf", required=False,
                        dest='tfidf_features', action='store_false')
    parser.add_argument("-t", "--test_percentage", help="percentage of data that is test data", required=False,
                        default=0.2, type=float)
    parser.add_argument("-u", "--unlabeled_percentage", help="percentage of unlabeled data", required=False,
                        default=0.6, type=float)
    parser.add_argument("-ppi", "--prediction_per_iteration",
                        help="percentage of additional training data per iteration",
                        required=False, default=0.1, type=cast_type_of_argument)
    parser.add_argument("-m", "--mode", help="Choose training mode", required=False,
                        choices=['active', 'traditional', 'both'], default='both')
    parser.add_argument("-s", "--labeling_schema", help="labeling schema for vt labels", required=False, default="zero",
                        choices=["zero", "ambiguous", "fifty_percent"])
    parser.add_argument("-k", "--kfold", help="Kfold value k", required=False, default=5, type=int)
    parser.add_argument("-b", "--balance", help="factor to balance amd and gplay apps", default=1.0, type=float,
                        required=False)
    return parser


argumentParser = defineArguments()
arguments = argumentParser.parse_args()
if not arguments.compare_virus_total:
    arguments.labeling_schema = "original"

# training_data, test_data, y_train, y_test, dataset_training, dataset_test = load_datasets(malware_features, goodware_features, arguments.test_percentage,
# training_data, test_data, y_train, y_test, dataset_training, dataset_test = load_datasets_for_validation(balance_factor=arguments.balance,
# training_data, test_data, y_train, y_test, dataset_training, dataset_test = load_datasets_ordered_swap_training(balance_factor=arguments.balance,
if arguments.tfidf_features:
    training_data, test_data, y_train, y_test, dataset_training, dataset_test = load_datasets_ordered(
        labeling_schema=arguments.labeling_schema, compare_vt=arguments.compare_virus_total,
        ignore_ambiguous=arguments.ignore_ambiguous)
else:
    training_data, test_data, y_train, y_test, dataset_training, dataset_test = load_datasets_engineered_features(
        labeling_schema=arguments.labeling_schema, compare_vt=arguments.compare_virus_total,
        ignore_ambiguous=arguments.ignore_ambiguous)
prettyPrint("Retrieved %s apps for training" % len(training_data))
print(y_train.count(0), y_train.count(1), y_test.count(0), y_test.count(1))
training_label, training_unlabeld, y_label, y_oracle = split_dataset(training_data, y_train,
                                                                     arguments.unlabeled_percentage)
prettyPrint("Start training with %s labeled data" % len(training_label))


def next_training_data(AL, clf):
    """
    get next training data based on active learning ranking
    :param AL: active learner
    :param clf: probabilistic classifier4
    :return: training data and corresponding label
    """
    ranking = AL.rank(clf, np.asarray(training_unlabeld), num_queries=arguments.prediction_per_iteration)
    ranking.sort()
    ranking = ranking[::-1]
    new_training_data, new_labels = [], []
    for index in ranking:
        new_training_data.append(training_unlabeld.pop(index))
        new_labels.append(y_oracle.pop(index))
    prettyPrint("Adding %s samples to training data" % len(new_training_data))
    return new_training_data, new_labels


def train_naive_bayes():
    prettyPrint("Welcome to the \"Aion\"'s naive active experiment with naive bayes")

    iteration = 1  # Initial values
    dummy_metrics = {"accuracy": 0.0, "recall": 0.0, "specificity": 0.0, "precision": 0.0, "f1score": 0.0,
                     "cross_val": 0.0}
    metrics_dict, metricsDict_test = {}, {}
    metrics_dict[0] = dummy_metrics
    metrics_dict[-1] = dummy_metrics

    AL = ActiveLearner(strategy=arguments.query_strategy)

    Xtr, ytr = training_label, y_label
    Xte, yte = test_data, y_test

    while (round(metrics_dict[iteration - 1]["f1score"] - metrics_dict[iteration - 2]["f1score"], 2) >=
           -(arguments.accuracymargin / 100.0)) and \
            (iteration <= int(arguments.iterations) and len(training_unlabeld) > 0):

        if iteration > 1:
            # Add suggested data to training data
            new_training_data, new_label = next_training_data(AL, clf)
            Xtr += new_training_data
            ytr += new_label

        ####################################
        # Train naive bayes classifier #
        ###################################
        cv_scores = []
        #prettyPrint("Classifying using Multinomial Naive Bayes")
        #clf, predicted, predicted_test, cv_scores = kFoldNB(arguments.kfold, Xtr, ytr, Xte, yte, alpha=arguments.alpha)
        clf, predicted, predicted_test = ScikitLearners.predictAndTestRandomForest(Xtr, ytr, Xte, yte,
                                                                                       estimators=100,
                                                                                       selectKBest=0)
        metrics = calculateMetrics_multiclass(ytr, predicted)
        metrics["cross_val"] = cv_scores
        metrics_test = calculateMetrics_multiclass(yte, predicted_test)
        metrics_dict[iteration] = metrics
        metricsDict_test[iteration] = metrics_test

        prettyPrint("F1-score in iteration %s" % str(iteration) + ": %s" % str(metrics_dict[iteration]["f1score"]))

        # Update the iteration number
        iteration += 1

    # Final Results
    del metrics_dict[-1]
    del metrics_dict[0]
    prettyPrint("Training results after %s iterations" % str(iteration - 1), "output")
    print_metrics(metrics_dict)

    prettyPrint("Test metrics")
    print_metrics(metricsDict_test)

    resultsDict = {'training': metrics_dict, "test": metricsDict_test}

    '''prettyPrint("Print 10 best feature for malware")
    mal_prob_sorted = clf.feature_log_prob_[0, :].argsort()
    find_best_features(mal_prob_sorted)
    prettyPrint("Print 10 best features for goodware")
    good_prob_sorted = clf.feature_log_prob_[1, :].argsort()
    find_best_features(good_prob_sorted)'''

    return resultsDict


def find_best_features(feature_ranking):
    id2token_dictionary = dictionary.Dictionary.load_from_text(corpus_path)
    tokenTuples = [tuple(map(int, i.split(' '))) for i in open(feature_mapping_path)]
    for i in range(10):
        featureIndex = feature_ranking[i]
        tokenKey = tokenTuples[featureIndex][0]
        print(featureIndex, tokenKey, id2token_dictionary[tokenKey])


def train_inactive_NB():
    prettyPrint("Welcome to the \"Aion\"'s naive inactive experiment with naive bayes")

    metricsDict = {}
    Xtr, ytr = training_data, y_train
    Xte, yte = test_data, y_test

    ####################################
    # Train naive bayes classifier #
    ###################################
    prettyPrint("Classifying using Multinomial Naive Bayes")
    cv_scores = []
    #clf, predicted, predicted_test, cv_scores = kFoldNB(arguments.kfold, Xtr, ytr, Xte, yte, alpha=arguments.alpha)
    clf, predicted, predicted_test = ScikitLearners.predictAndTestRandomForest(Xtr, ytr, Xte, yte,
                                                                               estimators=100,
                                                                               selectKBest=0)
    metrics = calculateMetrics_multiclass(ytr, predicted)
    metrics_test = calculateMetrics_multiclass(yte, predicted_test)
    metricsDict['training'] = metrics
    metricsDict['training']["cross_val"] = cv_scores
    metricsDict['test'] = metrics_test

    # Final Results
    prettyPrint("Training results")
    print_metrics(metricsDict)

    prettyPrint("Test metrics")
    print_metrics({'test': metricsDict['test']})

    return metricsDict


def print_metrics(metrics_dict):
    for m in metrics_dict:
        # The average metrics for training dataset
        prettyPrint("Metrics using " + str(m), "output")
        prettyPrint("Accuracy: %s" % str(metrics_dict[m]["accuracy"]), "output")
        prettyPrint("Recall: %s" % str(metrics_dict[m]["recall"]), "output")
        prettyPrint("Specificity: %s" % str(metrics_dict[m]["specificity"]), "output")
        prettyPrint("Precision: %s" % str(metrics_dict[m]["precision"]), "output")
        prettyPrint("F1 Score: %s" % str(metrics_dict[m]["f1score"]), "output")
        if "cross_val" in metrics_dict[m]:
            prettyPrint("Cross validation scores: %s" % str(metrics_dict[m]["cross_val"]), "output")


def non_default_arguments():
    argument_string = ''
    for arg in vars(arguments):
        if getattr(arguments, arg) != argumentParser.get_default(arg):
            argument_string += '-' + arg.capitalize() + str(getattr(arguments, arg)).capitalize()
    return argument_string


if __name__ == '__main__':
    arg_string = non_default_arguments()
    metricsDict = {'arguments': vars(arguments)}
    metricsDict['arguments']["training_data"] = dataset_training
    metricsDict['arguments']["test_data"] = dataset_test
    metricsDict['arguments']['training: num benign'] = y_train.count(0)
    metricsDict['arguments']['training: num malicious'] = y_train.count(1)
    if arguments.mode is 'active':
        metricsDict["active"] = train_naive_bayes()
    elif arguments.mode is 'traditional':
        metricsDict["inactive"] = train_inactive_NB()
    else:
        metricsDict["active"] = train_naive_bayes()
        metricsDict["inactive"] = train_inactive_NB()
    save_metrics_to_file(results_folder + 'naive_bayes' + arg_string + '.json', metricsDict)

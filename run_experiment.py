# coding=utf-8
import argparse
from utils.config import *
from utils.data_organisation import *
from utils.utils import *
from utils.alp.active_learning import ActiveLearner
from utils.learners import calculateMetrics, kFoldNB, kFoldDT


def cast_type_of_argument(arg):
    try:
        return int(arg)
    except ValueError:
        return float(arg)


def defineArguments():
    parser = argparse.ArgumentParser(prog="run_experiment.py",
                                     description="Train malware detection with active learning")
    parser.add_argument("-q", "--query_strategy", help="Query strategy for active learner",
                        required=False, default="least_confident",
                        choices=['entropy', 'max_margin', 'least_confident', 'random'])
    parser.add_argument("-a", "--alpha", help="Parameter alpha for naive bayes", required=False, default=1.0,
                        type=float)
    parser.add_argument("-d", "--depth", help="Max depth of decision tree", required=False, default=None, type=int)
    parser.add_argument("-msl", "--min_samples_leaf", required=False, default=1, type=int)
    parser.add_argument("-i", "--iterations", help="Maximum number of iterations", required=False, default=50, type=int)
    parser.add_argument("-acc", "--accuracymargin", help="accuracy margin for iterations", required=False, default=100,
                        type=float)
    parser.set_defaults(compare_virus_total=True, tfidf_features=True, nb=True)
    parser.add_argument("-e", "--engineered", help="Use engineered features instead of tfidf", required=False,
                        dest='tfidf_features', action='store_false')
    parser.add_argument("-n", "--not_nb", help="use different clf", dest="nb", action="store_false")
    parser.add_argument("-u", "--unlabeled_percentage", help="percentage of unlabeled data", required=False,
                        default=0.8, type=float)
    parser.add_argument("-ppi", "--prediction_per_iteration",
                        help="percentage of additional training data per iteration",
                        required=False, default=0.1, type=cast_type_of_argument)
    parser.add_argument("-s", "--labeling_scheme", help="labeling scheme for vt labels", required=False, default="zero",
                        choices=["zero", "fifty_percent", "original"])
    parser.add_argument("-k", "--kfold", help="Kfold value k", required=False, default=5, type=int)
    return parser


# parse user defined arguments
argumentParser = defineArguments()
arguments = argumentParser.parse_args()
arg_string = non_default_arguments(arguments, argumentParser)
# is labeling scheme is original, we will not compare the virusTotal reports
if arguments.labeling_scheme == "original":
    arguments.compare_virus_total = False

# load feature vectors from path
if arguments.tfidf_features:
    training_data, test_data, y_train, y_test, dataset_training, dataset_test = load_datasets_ordered(
        labeling_scheme=arguments.labeling_scheme, compare_vt=arguments.compare_virus_total)
else:
    training_data, test_data, y_train, y_test, dataset_training, dataset_test = load_datasets_engineered_features(
        labeling_scheme=arguments.labeling_scheme, compare_vt=arguments.compare_virus_total)
prettyPrint("Retrieved %s apps for training" % len(training_data))
# split training data in unlabeled and labeled subsets
training_label, training_unlabeld, y_label, y_oracle = split_dataset(training_data, y_train,
                                                                     arguments.unlabeled_percentage)
prettyPrint("Start training with %s labeled data" % len(training_label))


def next_training_data(AL, clf):
    """
    get next training data based on active learning ranking
    :param AL: active learner
    :param clf: probabilistic classifier4
    :return: training data and corresponding label
    """
    ranking = AL.rank(clf, np.asarray(training_unlabeld), num_queries=arguments.prediction_per_iteration)
    ranking.sort()
    ranking = ranking[::-1]
    new_training_data, new_labels = [], []
    for index in ranking:
        new_training_data.append(training_unlabeld.pop(index))
        new_labels.append(y_oracle.pop(index))
    prettyPrint("Adding %s samples to training data" % len(new_training_data))
    return new_training_data, new_labels


def train_active_classifier():
    """
    Train active classifier
    :return: dict with metrics of all iterations
    """
    prettyPrint("Welcome to the the active training")

    iteration = 1  # Set Initial values
    dummy_metrics = {"accuracy": 0.0, "recall": 0.0, "specificity": 0.0, "precision": 0.0, "f1score": 0.0,
                     "cross_val": 0.0}
    metrics_dict, metricsDict_test = {}, {}
    metrics_dict[0] = dummy_metrics
    metrics_dict[-1] = dummy_metrics

    # Instatiate active learner with defined query_strategy
    AL = ActiveLearner(strategy=arguments.query_strategy)

    Xtr, ytr = training_label, y_label
    Xte, yte = test_data, y_test

    while (round(metrics_dict[iteration - 1]["f1score"] - metrics_dict[iteration - 2]["f1score"], 2) >=
           -(arguments.accuracymargin / 100.0)) and \
            (iteration <= int(arguments.iterations) and len(training_unlabeld) > 0):

        if iteration > 1:
            # Add suggested data to training data
            new_training_data, new_label = next_training_data(AL, clf)
            Xtr += new_training_data
            ytr += new_label

        ####################################
        # Train classifier #
        ###################################
        if arguments.nb:
            prettyPrint("Classifying using Multinomial Naive Bayes")
            clf, predicted, predicted_test, cv_scores = kFoldNB(arguments.kfold, Xtr, ytr, Xte, yte,
                                                                alpha=arguments.alpha)
        else:
            prettyPrint("Classifying using Decision Tree")
            clf, predicted, predicted_test, cv_scores = kFoldDT(arguments.kfold, Xtr, ytr, arguments.depth,
                                                                arguments.min_samples_leaf, Xte, yte)
            save_decision_tree(clf, Xte, results_folder + 'tree' + arg_string + '_it' + str(iteration) + '.txt')
        # claculate metrics of classifier in respective iteration
        metrics = calculateMetrics(ytr, predicted)
        metrics["cross_val"] = cv_scores
        metrics_test = calculateMetrics(yte, predicted_test)
        metrics_dict[iteration] = metrics
        metricsDict_test[iteration] = metrics_test

        prettyPrint("F1-score in iteration %s" % str(iteration) + ": %s" % str(metrics_dict[iteration]["f1score"]))

        # Update the iteration number
        iteration += 1

    # Final Results: delete dummy metrics and report metrics
    del metrics_dict[-1]
    del metrics_dict[0]
    prettyPrint("Training results after %s iterations" % str(iteration - 1), "output")
    print_metrics(metrics_dict)

    prettyPrint("Test metrics")
    print_metrics(metricsDict_test)

    resultsDict = {'training': metrics_dict, "test": metricsDict_test}

    return resultsDict


def train_inactive_classifier():
    """
    train baseline classifier with same settings
    :return: dict with test and train results
    """
    prettyPrint("Welcome to the inactive baselien training")

    # instatiate variables
    metricsDict = {}
    Xtr, ytr = training_data, y_train
    Xte, yte = test_data, y_test

    ####################################
    # Train naive bayes classifier #
    ###################################
    if arguments.nb:
        prettyPrint("Classifying using Multinomial Naive Bayes")
        clf, predicted, predicted_test, cv_scores = kFoldNB(arguments.kfold, Xtr, ytr, Xte, yte, alpha=arguments.alpha)
    else:
        prettyPrint("Classifying using Decision Tree")
        clf, predicted, predicted_test, cv_scores = kFoldDT(arguments.kfold, Xtr, ytr, arguments.depth,
                                                            arguments.min_samples_leaf, Xte, yte)
        save_decision_tree(clf, Xte, results_folder + 'tree' + arg_string + '.txt')
        save_decision_tree_png(clf, arguments.tfidf_features, results_folder + 'plot_tree_' + arg_string + '.png')

    # calculate and report metrics
    metrics = calculateMetrics(ytr, predicted)
    metrics_test = calculateMetrics(yte, predicted_test)
    metricsDict['training'] = metrics
    metricsDict['training']["cross_val"] = cv_scores
    metricsDict['test'] = metrics_test

    # Final Results
    prettyPrint("Results of inactive classifier")
    print_metrics(metricsDict)

    return metricsDict


def print_metrics(metrics_dict):
    """
    Print the metricsof metrics_dict
    :param metrics_dict: metrics to print
    """
    for m in metrics_dict:
        prettyPrint("Metrics using " + str(m), "output")
        prettyPrint("Accuracy: %s" % str(metrics_dict[m]["accuracy"]), "output")
        prettyPrint("Recall: %s" % str(metrics_dict[m]["recall"]), "output")
        prettyPrint("Specificity: %s" % str(metrics_dict[m]["specificity"]), "output")
        prettyPrint("Precision: %s" % str(metrics_dict[m]["precision"]), "output")
        prettyPrint("F1 Score: %s" % str(metrics_dict[m]["f1score"]), "output")
        prettyPrint("MCC score: %s" % str(metrics_dict[m]["mcc"]), "output")
        if "cross_val" in metrics_dict[m]:
            prettyPrint("Cross validation scores: %s" % str(metrics_dict[m]["cross_val"]), "output")


if __name__ == '__main__':
    # Save metadata of training and trigger training
    metricsDict = {'arguments': vars(arguments)}
    # dataset_training and dataset_test defines the datasets used for training and test
    metricsDict['arguments']["training_data"] = dataset_training
    metricsDict['arguments']["test_data"] = dataset_test
    # save the amount of mailicious (label 1) and benign (label 0) samples in training data
    metricsDict['arguments']['training: num benign'] = y_train.count(0)
    metricsDict['arguments']['training: num malicious'] = y_train.count(1)
    # train active and inactive classifiers
    metricsDict["active"] = train_active_classifier()
    metricsDict["inactive"] = train_inactive_classifier()
    storename = 'naive_bayes' if arguments.nb else 'decision_tree'
    # save results and metadata of training to file
    save_metrics_to_file(results_folder + storename + arg_string + '.json', metricsDict)

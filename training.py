# Base: Aion.tools.runExperimentI.py!
# !/usr/bin/python
from utils.callgraph_windows.feature_generator import Feature_Generator
from utils.callgraph_windows.window_generator import next_training_data
from utils.config import *
import sys, glob, re, pickle
sys.path.insert(0, aion_path)
import Aion
from Aion.conf.config import *
Aion.conf.config.LOG_FILE = aion_log_file
from Aion.data_inference.learning import ScikitLearners
from Aion.utils.db import *
import argparse


def defineArguments():
    parser = argparse.ArgumentParser(prog="runExperimentI.py",
                                     description="A tool to implement the stimulation-detection feedback loop using Garfield as stimulation engine.")
    parser.add_argument("-y", "--validation",
                        help="Instructs Aion how to perform validation i.e. on 'training' or 'validation' datasets",
                        required=False, default="training", choices=["training", "validation"])
    parser.add_argument("-k", "--kfold",
                        help="Whether to use k-fold cross validation and the value of \"K\". Valid for 'validation' type of 'validation'",
                        required=False, default=2)
    parser.add_argument("-s", "--selectkbest",
                        help="Whether to select K best features from the ones extracted from the APK's", required=False,
                        default=0)
    parser.add_argument("-e", "--featuretype", help="The type of features to consider during training", required=False,
                        default="hybrid", choices=["static", "dynamic", "hybrid"])
    parser.add_argument("-p", "--fileextension", help="The extension of feature files", required=False, default="num")
    parser.add_argument("-m", "--accuracymargin",
                        help="The margin (in percentage) within which the training accuracy is allowed to dip",
                        required=False, default=1)
    parser.add_argument("-i", "--maxiterations", help="The maximum number of iterations to allow", required=False,
                        default=1)
    return parser

def load_data_from_source(subpath):
    data = []
    for graph_path in glob.glob(export_path + subpath, '*_graph.pkl'):
        with open(re.sub('_graph', '', graph_path), 'r') as fh:
            apk = pickle.load(fh)
        apk.load_graph(graph_path)
        print('loaded', apk.package)
        data.append(apk)
    return apk


def train(training_data, test_data, feature_gen):
    try:
        argumentParser = defineArguments()
        arguments = argumentParser.parse_args()
        prettyPrint("Welcome to the \"Aion\"'s window experiment I")

        iteration = 1  # Initial values
        reanalysis = False
        appTraces = {}
        currentMetrics = {"accuracy": 0.0, "recall": 0.0, "specificity": 0.0, "precision": 0.0, "f1score": 0.0}
        previousMetrics = {"accuracy": -1.0, "recall": -1.0, "specificity": -1.0, "precision": -1.0, "f1score": -1.0}
        reanalyzeMalware, reanalyzeGoodware = [], []  # Use this as a cache until conversion

        while (round(currentMetrics["f1score"] - previousMetrics["f1score"], 2) >=
               -(float(arguments.accuracymargin) / 100.0)) and (iteration <= int(arguments.maxiterations)):
            # Set/update the reanalysis flag
            reanalysis = True if iteration > 1 else False
            prettyPrint("Experiment I: iteration #%s" % iteration, "info2")
            Xtr, ytr = next_training_data(training_data, feature_gen) if not reanalysis \
                else next_training_data(reanalyzeAPKs, feature_gen)
            Xte, yte = next_training_data(test_data, feature_gen)

            metricsDict, metricsDict_test = {}, {}
            ####################################
            # Ensemble of learning algorithms #
            ###################################
            prettyPrint("Ensemble mode classification: K-NN, SVM, and Random Forests")
            predicted_trees100 = []
            # Classifying using K-nearest neighbors
            K = [10, 25, 50, 100, 250, 500]
            for k in K:
                prettyPrint("Classifying using K-nearest neighbors with K=%s" % k)
                clf, predicted, predicted_test = ScikitLearners.predictAndTestKNN(Xtr, ytr, Xte, yte, K=k,
                                                                                  selectKBest=int(
                                                                                      arguments.selectkbest))
                metrics = ScikitLearners.calculateMetrics(ytr, predicted)
                metrics_test = ScikitLearners.calculateMetrics(yte, predicted_test)
                metricsDict["KNN%s" % k] = metrics
                metricsDict_test["KNN%s" % k] = metrics_test

            # Classifying using Random Forests
            E = [10, 25, 50, 75, 100]
            for e in E:
                prettyPrint("Classifying using Random Forests with %s estimators" % e)
                clf, predicted, predicted_test = ScikitLearners.predictAndTestRandomForest(Xtr, ytr, Xte, yte,
                                                                                           estimators=e,
                                                                                           selectKBest=int(
                                                                                               arguments.selectkbest))
                if e == 100:
                    predicted_trees100 = [] + predicted_test
                metrics = ScikitLearners.calculateMetrics(ytr, predicted)
                metrics_test = ScikitLearners.calculateMetrics(yte, predicted_test)
                metricsDict["Trees%s" % e] = metrics
                metricsDict_test["Trees%s" % e] = metrics_test

            # Classifying using SVM
            prettyPrint("Classifying using Support vector machines")
            clf, predicted, predicted_test = ScikitLearners.predictAndTestSVM(Xtr, ytr, Xte, yte,
                                                                              selectKBest=int(arguments.selectkbest))
            metrics = ScikitLearners.calculateMetrics(ytr, predicted)
            metrics_test = ScikitLearners.calculateMetrics(yte, predicted_test)
            metricsDict["SVM"] = metrics
            metricsDict_test["SVM"] = metrics_test

            # Now do the majority voting ensemble
            allCs = ["KNN-%s" % x for x in K] + ["FOREST-%s" % e for e in E] + ["SVM"]
            clf, predicted, predicted_test = ScikitLearners.predictAndTestEnsemble(Xtr, ytr, Xte, yte,
                                                                                   classifiers=allCs, selectKBest=int(
                                                                                    arguments.selectkbest))
            metrics = ScikitLearners.calculateMetrics(predicted, ytr)  # Used to decide upon whether to iterate more
            metrics_test = ScikitLearners.calculateMetrics(predicted_test, yte)
            metricsDict["Ensemble"] = metrics
            metricsDict_test["Ensemble"] = metrics_test

            # Print and save results
            for m in metricsDict:
                # The average metrics for training dataset
                prettyPrint("Metrics using %s-fold cross validation and %s" % (arguments.kfold, m), "output")
                prettyPrint("Accuracy: %s" % str(metricsDict[m]["accuracy"]), "output")
                prettyPrint("Recall: %s" % str(metricsDict[m]["recall"]), "output")
                prettyPrint("Specificity: %s" % str(metricsDict[m]["specificity"]), "output")
                prettyPrint("Precision: %s" % str(metricsDict[m]["precision"]), "output")
                prettyPrint("F1 Score: %s" % str(metricsDict[m]["f1score"]), "output")

            # Save incorrectly-classified training instances for re-analysis
            reanalyzeAPKs = []  # Reset the lists to store new misclassified instances
            for index in range(len(ytr)):
                if predicted[index] != ytr[index]:
                    reanalyzeAPKs.append(training_data[index])

            prettyPrint(
                "Reanalyzing %s benign apps and %s malicious apps" % (len(reanalyzeGoodware), len(reanalyzeMalware)),
                "debug")

            # Swapping metrics
            previousMetrics = currentMetrics
            currentMetrics = metricsDict["Ensemble"]

            # Print and save results [FOR THE TEST DATASET]
            for m in metricsDict_test:
                # The average metrics for training dataset
                prettyPrint("Metrics using %s-fold cross validation and %s" % (arguments.kfold, m), "output")
                prettyPrint("Accuracy: %s" % str(metricsDict_test[m]["accuracy"]), "output")
                prettyPrint("Recall: %s" % str(metricsDict_test[m]["recall"]), "output")
                prettyPrint("Specificity: %s" % str(metricsDict_test[m]["specificity"]), "output")
                prettyPrint("Precision: %s" % str(metricsDict_test[m]["precision"]), "output")
                prettyPrint("F1 Score: %s" % str(metricsDict_test[m]["f1score"]), "output")

            # Update the iteration number
            iteration += 1

        # Final Results
        prettyPrint("Training results after %s iterations" % str(iteration - 1), "output")
        prettyPrint("Accuracy: %s" % currentMetrics["accuracy"], "output")
        prettyPrint("Recall: %s" % currentMetrics["recall"], "output")
        prettyPrint("Specificity: %s" % currentMetrics["specificity"], "output")
        prettyPrint("Precision: %s" % currentMetrics["precision"], "output")
        prettyPrint("F1 Score: %s" % currentMetrics["f1score"], "output")

    except FloatingPointError as e:
        prettyPrintError(e)
        return False

    prettyPrint("Good day to you ^_^")
    return True


def main():
    # Load APK's and split into training and test datasets
    # training_data, test_data = preprocess.preprocess()
    # TODO: load preprocessed data
    training_data = load_data_from_source('training_data/')
    test_data = load_data_from_source('test_data/')
    feature_gen = Feature_Generator(feature_mapping_path)

    train(training_data, test_data, feature_gen)


if __name__ == "__main__":
    main()

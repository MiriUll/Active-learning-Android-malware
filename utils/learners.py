from __future__ import print_function
from sklearn.naive_bayes import MultinomialNB
import numpy
from sklearn import tree
from sklearn.model_selection import cross_val_score
from utils import *

from sklearn.metrics import *


def specificity_score(ground_truth, predicted, classes=(1, 0)):
    try:
        if len(ground_truth) != len(predicted):
            return -1
        positive, negative = classes[0], classes[1]
        tp, tn, fp, fn = 0, 0, 0, 0
        for index in range(len(ground_truth)):
            if ground_truth[index] == negative and predicted[index] == negative:
                tn += 1
            elif ground_truth[index] == negative and predicted[index] == positive:
                fp += 1
            elif ground_truth[index] == positive and predicted[index] == negative:
                fn += 1
            else:
                tp += 1

        score = float(tn)/(float(tn)+float(fp))

    except Exception as e:
        print(e)
        return -1

    return score


def calculateMetrics(truth, predicted):
    """
    Calculates and returns a set of metrics from ground truth and predicted vectors
    :param truth: A list of ground truth labels
    :type truth: list
    :param predicted: A list of predicted labels
    :type predicted: list
    :return: A dict of metrics including accuracy, recall, specificity, precision, and F1-score
    """
    try:
        # Sanity check
        if not len(truth) == len(predicted):
            prettyPrint("The two vectors have different dimensionality", "warning")
            return {}

        # Calculate different mterics
        metrics = {"accuracy": accuracy_score(truth, predicted),
                   "recall": recall_score(truth, predicted),
                   "specificity": specificity_score(truth, predicted),
                   "precision": precision_score(truth, predicted),
                   "f1score": f1_score(truth, predicted),
                   "mcc": matthews_corrcoef(truth, predicted)}
    except Exception as e:
        prettyPrintError(e)
        return {}

    return metrics


def predictAndTestNB(X, y, Xtest=[], ytest=[], alpha=1.0):
    """
    Train naive bayes classifier
    :param X: Training data
    :param y: training labels
    :param Xtest: test data
    :param ytest: test labels
    :param alpha: parameter for naive bayes classifier
    :return: Trained classifer, predicted training and test labels
    """
    try:
        # Define classifier
        clf = MultinomialNB(alpha=alpha)
        # Start the cross validation learning
        X, y, Xtest, ytest = numpy.array(X), numpy.array(y), numpy.array(Xtest), numpy.array(ytest)
        # Fit model
        clf.fit(X, y)
        # test model
        predicted = clf.predict(X)
        if 1 < len(Xtest) == len(ytest) > 1:
            predicted_test = clf.predict(Xtest)
        else:
            predicted_test = []

    except Exception as e:
        print(e)
        return None, [], []

    return clf, predicted, predicted_test


def kFoldNB(kfold, X, y, Xtest=[], ytest=[], alpha=1.0):
    """
    Train naive bayes classifier and cross validate training results
    :param kfold: parameter k for k-fold validation
    :param X: Training data
    :param y: training labels
    :param Xtest: test data
    :param ytest: test labels
    :param alpha: parameter for naive bayes classifier
    :return: trained classifier, predicted training and test labels, cross_validation scores
    """

    clf, predicted, predicted_test = predictAndTestNB(X, y, Xtest, ytest, alpha)

    cv_scores = list(cross_val_score(clf, X, y, cv=kfold))

    return clf, predicted, predicted_test, cv_scores


def predictAndTestDT(X, y, max_depth, min_samples_leaf, Xtest=[], ytest=[]):
    """

    :param X: Training data
    :param y: training labels
    :param Xtest: test data
    :param ytest: test labels
    :param max_depth: maximum depth for trained tree
    :param min_samples_leaf: minimum samples per leaf node
    :return: Trained classifer, predicted training and test labels
    """
    try:
        # Define classifier and cross validation iterator
        clf = tree.DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf)
        # Train classifier
        X, y, Xtest, ytest = numpy.array(X), numpy.array(y), numpy.array(Xtest), numpy.array(ytest)
        # Fit model
        clf.fit(X, y)
        # predict labels and test model
        predicted = clf.predict(X)
        if 1 < len(Xtest) == len(ytest) > 1:
            predicted_test = clf.predict(Xtest)
        else:
            predicted_test = []

    except Exception as e:
        print(e)
        return None, [], []

    return clf, predicted, predicted_test


def kFoldDT(kfold, X, y, max_depth, min_samples_leaf, Xtest=[], ytest=[]):
    """
    Train decision tree classifier and cross validate training results
    :param kfold: parameter k for k-fold validation
    :param X: Training data
    :param y: training labels
    :param Xtest: test data
    :param ytest: test labels
    :param max_depth: maximum depth of tree
    :param min_samples_leaf: minimum samples per lead node
    :return: trained classifier, predicted training and test labels, cross_validation scores
    """

    clf, predicted, predicted_test = predictAndTestDT(X, y, max_depth, min_samples_leaf, Xtest, ytest)

    cv_scores = list(cross_val_score(clf, X, y, cv=kfold))

    return clf, predicted, predicted_test, cv_scores

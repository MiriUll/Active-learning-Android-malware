import glob
import random
import numpy as np


def load_datasets(maldirs, gooddirs, percent_test, feature_regex='*.tfidf'):
    """
    generate labeled training and test data sets
    :param feature_regex: regex to filter feature types, default *.tfidf
    :param maldirs: list of malware paths
    :param gooddirs: list of goodware paths
    :param percent_test: percentage of test data
    :return:
    """
    # load good- and malware features from paths
    all_mal_features, y = load_features_from_path(maldirs, 1, feature_regex)
    all_good_features, y2 = load_features_from_path(gooddirs, 0, feature_regex)
    y += y2
    all_features = all_mal_features + all_good_features

    # randomize order (use zip to remember feature <-> label mapping)
    z = zip(all_features, y)
    random.shuffle(z)
    all_features, y = list(zip(*z))

    # divide data into train and test set
    training_data, test_data, y_train, y_test = split_dataset(list(all_features), list(y), percent_test)

    return training_data, test_data, y_train, y_test


def load_features_from_path(paths, label, criterion):
    """
    load tfidf features from path system
    :param criterion: feature data type
    :param paths: list of paths to load data from, important: path must end with /
    :param label: label to label data with
    :return:
    """
    feature_list = []
    for path in paths:
        feature_list += [np.asarray(eval(open(f).read())) for f in list(glob.glob(path + criterion))]
    return feature_list, [label] * len(feature_list)


def split_dataset(data, labels, percentage):
    num_split = int(round(percentage * len(data)))
    data_split, y_split = data[:num_split], labels[:num_split]
    data_rest, y_rest = data[num_split:], labels[num_split:]
    return data_rest, data_split, y_rest, y_split

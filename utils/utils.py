import glob
import random
import numpy as np
import json

vt_reports_path = '/home/aesalem/Research/dejavu/data/vt_reports/'
package_hash_mapping = '/home/aesalem/Research/dejavu/data/lookup_structs/package_to_hash.txt'


def load_datasets_ordered(compare_vt=True, ignore_ambiguous=True):
    """
    generate labeled training and test data sets based on existing datasets
    :return: training, test data set and corresponding labels
    """
    datasets_training = ['google_play', 'amd']
    source_code_dir = '/home/miriam/malwaredetection/utils/source_codes/'
    training_data, y_train = load_features_from_path([source_code_dir + 's.home.aesalem.Research.Android.Datasets.Goodware.google_play./'],
                                                     0, compare_vt=compare_vt, ignore_ambiguous=ignore_ambiguous)
    t, y = load_features_from_path([source_code_dir + 's.home.aesalem.Research.Android.Datasets.Malware.AMD.amd_data./']
                                   , 1, compare_vt=compare_vt, ignore_ambiguous=ignore_ambiguous)
    t = t[:len(training_data)]
    y = y[len(training_data)]
    training_data += t
    y_train += y

    datasets_test = ['malgenome']
    test_data, y_test = load_features_from_path([source_code_dir + 's.home.aesalem.Research.Android.Datasets.Malware.MalGenome.genome./'],
                                                1, compare_vt=compare_vt, ignore_ambiguous=ignore_ambiguous)
    '''t, y = load_features_from_path([source_code_dir + 's.home.aesalem.Research.Android.Datasets.Malware.AndroZoo_Piggybacking.piggybacked./'], 1, compare_vt=compare_vt, ignore_ambiguous=ignore_ambiguous)
    test_data += t
    y_test += y
    t, y = load_features_from_path([source_code_dir + 's.home.aesalem.Research.Android.Datasets.Malware.AndroZoo_Piggybacking.original./'], 0, compare_vt=compare_vt, ignore_ambiguous=ignore_ambiguous)
    test_data += t
    y_test += y'''

    training_data, y_train = randomize_dataset(training_data, y_train)

    return training_data, test_data, y_train, y_test, datasets_training, datasets_test


def load_datasets(maldirs, gooddirs, percent_test, feature_regex='*.tfidf'):
    """
    generate labeled training and test data sets with random training - test data
    :param feature_regex: regex to filter feature types, default *.tfidf
    :param maldirs: list of malware paths
    :param gooddirs: list of goodware paths
    :param percent_test: percentage of test data
    :return: training, test data set and corresponding labels
    """
    # load good- and malware features from paths
    all_mal_features, y = load_features_from_path(maldirs, 1, feature_regex)
    all_good_features, y2 = load_features_from_path(gooddirs, 0, feature_regex)
    y += y2
    all_features = all_mal_features + all_good_features

    # randomize order (use zip to remember feature <-> label mapping)
    all_features, y = randomize_dataset(all_features, y)

    # divide data into train and test set
    training_data, test_data, y_train, y_test = split_dataset(list(all_features), list(y), percent_test)

    return training_data, test_data, y_train, y_test


def load_features_from_path(paths, label, compare_vt=True, ignore_ambiguous=True, criterion='*.tfidf'):
    """
    load tfidf features from path system
    :param ignore_ambiguous: true if ambiguous vt reports are ignored for training
    :param compare_vt: False if labels arent compared to vt reports
    :param criterion: feature data type
    :param paths: list of paths to load data from, important: path must end with /
    :param label: label to label data with
    :return:
    """
    package_to_hash = load_hash_package_mapping()
    if isinstance(paths, str):
        paths = [paths]
    feature_list, labels = [], []
    for path in paths:
        print('loading features in path ' + path)
        for f in list(glob.glob(path + criterion)):
            if compare_vt:
                package = f.rsplit('/', 1)[1].split('.tfidf')[0]
                label = label_from_vt_report(package_to_hash, package, label)
                if ignore_ambiguous and label < 0:
                    continue
            feature_list += [np.asarray(eval(open(f).read()))]
            labels.append(label)
    return feature_list, labels


def randomize_dataset(all_features, y):
    """
    shuffle dataset and keep feature -> label mapping
    :param all_features:
    :param y:
    :return:
    """
    z = zip(all_features, y)
    random.shuffle(z)
    all_features, y = list(zip(*z))
    return list(all_features), list(y)


def load_hash_package_mapping():
    with open(package_hash_mapping, 'r') as inf:
        dict_from_file = eval(inf.read())
    return dict_from_file


def label_from_vt_report(package_to_hash, package, backup_label):
    """
    try to load label from vt, else use backup label
    :param package_to_hash: mapping package -> hash
    :param package: package of app
    :param backup_label: backup label if there is no vt_report
    :return: label to classify app
    """
    try:
        vt_report = eval(open(vt_reports_path + package_to_hash[package] + '.report').read())
        pos = vt_report['positives']
        #print(backup_label, pos, vt_report["total"])
        if pos == 0:
            return 0
        elif pos / float(vt_report["total"]) >= 0.50:
            return 1
        else:
            return -1
    except KeyError:
        return backup_label

def split_dataset(data, labels, percentage):
    """
    split dataset into subsets
    :param data: dataset as list to split
    :param labels: labels of dataset as list
    :param percentage: percentage of split data
    :return: two datasets and corresponding labels
    """
    num_split = int(round(percentage * len(data)))
    data_split, y_split = data[:num_split], labels[:num_split]
    data_rest, y_rest = data[num_split:], labels[num_split:]
    return data_rest, data_split, y_rest, y_split


def save_metrics_to_file(path, metricsDict):
    with open(path, 'w+') as f:
        json.dump(metricsDict, f)

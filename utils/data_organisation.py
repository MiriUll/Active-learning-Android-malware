# coding=utf-8
import glob
import random
import re
import numpy as np

vt_reports_path = '/home/aesalem/Research/dejavu/data/vt_reports/'
package_hash_mapping = '/home/aesalem/Research/dejavu/data/lookup_structs/package_to_hash.txt'


def load_datasets_ordered(labeling_scheme, compare_vt):
    """
    generate labeled training and test data sets based on existing datasets
    use tfidf features
    :return: training, test data set and corresponding labels
    """
    datasets_training = ['google_play', 'amd_all']
    source_code_dir = '/home/miriam/malwaredetection/utils/source_codes/training_subset/'
    training_data, y_train = load_features_from_path(
        [source_code_dir + 's.home.aesalem.Research.Android.Datasets.Goodware.google_play./'],
        0, labeling_scheme, compare_vt=compare_vt)
    t, y = load_features_from_path([source_code_dir + 'amd/'], 1, labeling_scheme, compare_vt=compare_vt)
    training_data += t
    y_train += y

    datasets_test = ['piggybacking_orig', 'piggybacking_backed']
    test_data, y_test = load_features_from_path(
        [source_code_dir + 's.home.aesalem.Research.Android.Datasets.Malware.AndroZoo_Piggybacking.piggybacked./'], 1,
        labeling_scheme, compare_vt=compare_vt)
    t, y = load_features_from_path(
        [source_code_dir + 's.home.aesalem.Research.Android.Datasets.Malware.AndroZoo_Piggybacking.original./'], 0,
        labeling_scheme, compare_vt)
    test_data += t
    y_test += y

    # shuffle the order of training data
    training_data, y_train = randomize_dataset(training_data, y_train)

    return training_data, test_data, y_train, y_test, datasets_training, datasets_test


def load_datasets_engineered_features(labeling_scheme, compare_vt):
    """
    generate labeled training and testdata with engineered features based on datasets
    :param labeling_scheme:
    :param compare_vt:
    :return:
    """
    datasets_training = ['google_play_static', 'amd_subset_static']
    source_code_dir = '/home/aesalem/Research/dejavu/data/feature_vectors/'
    training_data, y_train = load_features_from_path([source_code_dir + 'gplay/static/'], 0, labeling_scheme,
                                                     compare_vt, criterion='*.static')
    t, y = load_fixed_subset(1, labeling_scheme, compare_vt)
    training_data += t
    y_train += y

    training_data, y_train = randomize_dataset(training_data, y_train)

    datasets_test = ['piggybacking_orig_static', 'piggybacking_backed_static']
    test_data, y_test = load_features_from_path([source_code_dir + 'piggybacked_only/static/'], 1, labeling_scheme,
                                                compare_vt, criterion='*.static')
    t, y = load_features_from_path([source_code_dir + 'original_only/static/'], 0, labeling_scheme, compare_vt,
                                   criterion='*.static')
    test_data += t
    y_test += y

    return training_data, test_data, y_train, y_test, datasets_training, datasets_test


def load_datasets(maldirs, gooddirs, percent_test, labeling_scheme, compare_vt,
                  feature_regex='*.tfidf'):
    """
    generate labeled training and test data sets with random training - test data
    :param compare_vt: boolean if original labels are overwritten by vt_reports
    :param labeling_scheme: scheme to decide if vt reports malicious
    :param feature_regex: regex to filter feature types, default *.tfidf
    :param maldirs: list of malware paths
    :param gooddirs: list of goodware paths
    :param percent_test: percentage of test data
    :return: training, test data set and corresponding labels
    """
    # load good- and malware features from paths
    all_mal_features, y = load_features_from_path(maldirs, 1, labeling_scheme, compare_vt=compare_vt,
                                                  criterion=feature_regex)
    all_good_features, y2 = load_features_from_path(gooddirs, 0, labeling_scheme, criterion=feature_regex,
                                                    compare_vt=compare_vt)
    y += y2
    all_features = all_mal_features + all_good_features

    # randomize order (use zip to remember feature <-> label mapping)
    all_features, y = randomize_dataset(all_features, y)

    # divide data into train and test set
    training_data, test_data, y_train, y_test = split_dataset(list(all_features), list(y), percent_test)

    return training_data, test_data, y_train, y_test, 'random', 'random'


def load_features_from_path(paths, label, labeling_scheme, compare_vt, criterion='.tfidf'):
    """
    load feature vectors from path system
    :param compare_vt: False if labels arent compared to vt reports
    :param criterion: feature data type
    :param paths: list of paths to load data from, important: path must end with /
    :param label: label to label data with
    :param labeling_scheme: scheme to compare vt reports
    :return:
    """
    if compare_vt:
        package_to_hash = load_hash_package_mapping()
    if isinstance(paths, str):
        paths = [paths]
    feature_list, labels = [], []
    for path in paths:
        print('loading features in path ' + path)
        for f in list(glob.glob(path + '*' + criterion)):
            if compare_vt:
                package = f.rsplit('/', 1)[1].split(criterion)[0]
                label = label_from_vt_report(package_to_hash, package, label, labeling_scheme)
            feature_list += [np.asarray(eval(open(f).read()))]
            labels.append(label)
    return feature_list, labels


def load_fixed_subset(label, labeling_scheme, compare_vt, fileextension='.static'):
    """
    load fixed amd subset of engineered features
    :param label: backup label
    :param labeling_scheme: scheme to label based on vt reports
    :param compare_vt: true if vt report labels are used
    :param fileextension: fileextension of saved vectors
    :return: training dataset and corresponding labels
    """
    if compare_vt:
        package_to_hash = load_hash_package_mapping()
    print('Load amd subset features')
    path = '/home/aesalem/Research/dejavu/data/feature_vectors/amd/static/'
    feature_list, labels = [], []
    ignored = 0
    with open('/home/miriam/malwaredetection/utils/source_codes/training_amd/fixed_amd_subset_hashes.txt', 'r') as f:
        for line in f:
            line = line.strip('\n')
            if compare_vt:
                label = label_from_vt_report(package_to_hash, line, label, labeling_scheme)
            try:
                feature_list += [np.asarray(eval(open(path + line + fileextension).read()))]
                labels.append(label)
            except IOError:
                ignored += 1
    print('Ignored %s files' % ignored)
    return feature_list, labels


def randomize_dataset(all_features, y):
    """
    shuffle dataset and keep feature -> label mapping
    :param all_features: feature vectors
    :param y: labels
    :return: randomised dataset
    """
    z = zip(all_features, y)
    random.shuffle(z)
    all_features, y = list(zip(*z))
    return list(all_features), list(y)


def load_hash_package_mapping():
    """
    load app package to hash mapping
    :return: dict
    """
    with open(package_hash_mapping, 'r') as inf:
        dict_from_file = eval(inf.read())
    return dict_from_file


def label_from_vt_report(package_to_hash, package, backup_label, scheme):
    """
    try to load label from vt, else use backup label
    :param package_to_hash: mapping package -> hash
    :param package: package of app
    :param backup_label: backup label if there is no vt_report
    :param scheme: scheme to compare vt reports
    :return: label to classify app
    """
    try:
        if re.match('^[A-Fa-f0-9]{64}$', package):
            app_report = package + '.report'
        else:
            app_report = package_to_hash[package] + '.report'
        vt_report = eval(open(vt_reports_path + app_report).read())
        pos = vt_report['positives']
        label = backup_label
        if scheme == 'fifty_percent':
            # magority voting of vt scanners
            label = 1 if pos / float(vt_report["total"]) >= 0.50 else 0
        elif scheme == 'zero':
            # malicious of at least one scanner deems app malicious
            label = 0 if pos / float(vt_report["total"]) <= 0.05 else 1
        # print(scheme, label)
        return label
    except KeyError:
        return backup_label


def split_dataset(data, labels, percentage):
    """
    split dataset into subsets
    :param data: dataset as list to split
    :param labels: labels of dataset as list
    :param percentage: percentage of split data
    :return: two datasets and corresponding labels
    """
    num_split = int(round(percentage * len(data)))
    data_split, y_split = data[:num_split], labels[:num_split]
    data_rest, y_rest = data[num_split:], labels[num_split:]
    return data_rest, data_split, y_rest, y_split
